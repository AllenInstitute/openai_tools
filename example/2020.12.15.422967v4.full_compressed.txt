A standardized non-visual behavioral event, such as a fidget, can impact the activity of primary sensory areas like V1 without modulating visual responses. In this study, the authors leveraged a standardized and spontaneous behavioral fidget event in passively viewing mice to dissect how these behavior signals are broadcasted to different layers and areas of the visual cortex. A large two-photon Ca2+ imaging database of neuronal responses uncovered four neural response types during fidgets that were surprisingly consistent in their proportion and response patterns across all visual areas and layers of the visual cortex. The layer and area identity could not be decoded above chance level based only on neuronal recordings. In contrast to running behavior, fidget-evoked neural responses were independent of visual processing. The broad availability of visually orthogonal standardized behavior signals could be a key component in how the cortex selects, learns, and binds local sensory information with motor outputs. Contrary to relevant motor outputs, irrelevant motor signals would use a separate neural subspace.While the brain-wide impact of behavioral events is now established, our analysis revealed that neurons in three different cortical layers and four visual areas have homogenous post-fidget neural responses. Fidget response profiles were stereotypical and equally distributed among 4 response types. In addition, in contrast to running behaviors, visually evoked neuronal responses showed no interaction with fidget neural events.

Results

Fidget as a standardized behavior output

We first sought to characterize the range of behavioral events that mice displayed under head fixation. While we recorded neuronal activity in vivo using two-photon imaging, mice were free to run on a rotating disc while a camera captured mouse body posture. We observed a variety of behaviors such as whisking, grooming, mastication, flailing (uncoordinated movement), walking, running, and a startle behavior we denote as a “fidget”. Fidgets manifested as a combination of abdominal flexion (causing the abdomen to be raised above the rotating disk) and an upward force generated from the lower limbs causing lower trunk curvature and contraction. Fidgets were qualitatively stereotyped across mice in their duration, pattern of movement, and motor response magnitude. Following this observation, we sought to develop a computer vision model to automatically identify fidget events from hours of mice behavioral videos. Six human annotators first established a training dataset (20 mice, 10,000 fidgets manually annotated, see Methods). We computed the Histograms of Oriented Gradients (HOGs) for each video frame and concatenated a feature vector from a one-second section of frames (30 frames). HOGs are transformation invariant visual features extracted using edge-detection-like computation. HOG features are largely invariant to variation in lighting conditions and image transformations such as translation, rotation, scale. This allows us to carry out robust behavioral feature detection as others have firmly established. Another advantage of using HOG vectors is the biologically inspired emphasis on interpretable edge detection computations and has been found to be superior to Eigenfeature-based face recognition models. Using this feature, we trained a Support Vector Machine (SVM) classifier in a supervised manner using the human-annotated labels. Our final trained model had a recall performance of 74%+/-4.2 (mean+/-std, n=7) and a precision performance of 78%+/-5.3 (mean+/-std, n=7) for the seven one-hour long experiments held out as a validation test-set. Our trained classifier was as accurate at identifying fidgets and other mouse behaviors as human annotators. Indeed, seven pairs of annotators analyzed the same videos and their annotations were compared head-to-head. Each video was drawn from a seven video validation test-set. Head-to-head human vs. human performance recall for the seven videos was 73%+/-5.9 (mean+/-std, n=7) and had a performance precision of 74%+/-7.2 (mean+/-std, n=7); this was within the range of the model’s performance. Having established a robust computer vision model, we automatically annotated 144 one-hour experiments total (recall p=0.51, performance p=0.26).

To quantify the standardization of fidget events across mice, we integrated the optical flow magnitude of the fidget motor response (see Methods) over the duration of the fidget. 80% of all fidget events from 20 one-hour experiments and across 20 unique mice fell within 30% of the maximum magnitude; this consistency reaffirmed the stereotypy of fidget events. 

Occurrence of fidget across mice and visual stimuli

We next sought to establish whether the occurrence of fidgets could relate to our visual stimuli. Fidget behavior has been associated with stress and surprise responses in mice. As described in a previous publication, mice passively viewed a range of both artificial (drifting and static gratings, locally sparse noise) and natural visual stimuli (natural scenes and natural movies), organized into three different recording sessions (sessions A, B, and C). We hypothesized that artificial visual stimuli (e.g. drifting gratings) induce a more stressful or surprising context than natural stimuli that are more ethologically familiar to the mouse (e.g. natural movies). In particular, the moving drifting gratings, through its perceived motion, evoke an innate avoidance response. In line with our prediction, the average normalized fidget rate was significantly higher during drifting gratings (p=0.023, two-tailed t-test, n=60) than other stimuli. 

We found that fidget rate was highly variable across mice, raising the possibility that various mouse Cre-lines have different stress sensitivities, thus accounting for the fidget rate variance. The absolute fidget rate did not significantly differ between mice from different Cre-lines (ANOVA p-value=0.14, n=144). This result did not exclude that mice could be more sensitive to individual stimuli. To account for variability across individual mice, we normalized the change in fidget rate evoked during the session with drifting gratings (session A) by the fidget rate during session C. Similarly to the absolute rate, we saw non-significant changes (ANOVA p-value=0.14, n=144). 

Previous research has shown that fidget behaviors can be learned. Our passive viewing protocol included two weeks of habituation to our visual stimuli (see Methods), suggesting we could have reached a more stable state. To check whether the fidget occurrence we see is learned over the course of our two-photon experiments, we quantified the average fidget rate across mice as a function of the number of visual stimulation sessions the mouse has already seen. We found a non-significant change in the fidget rate with an increased number of sessions experienced.In this study, the authors investigated the neuronal correlates of fidgeting in mice using genetically encoded calcium sensors and two-photon calcium imaging. They found that many neurons showed a robust and prolonged response after fidget onset, and these responses fell into four distinct types: neutral, phasic, active, and depressed. The authors also found that the distribution of neural response types was surprisingly consistent across different layers and areas of the visual cortex. Furthermore, they found that post-fidget neuronal responses did not differ between layers or areas, suggesting that fidgeting does not affect visual responses. Overall, these findings suggest that a standardized behavior equally impacts the cortex and that fidgeting may not be behaviorally relevant to vision.Todoso, we compared the response of cells to different drifting grating orientations and spatial frequencies during baseline condition with during fidget. We found that all 4 sub-types of fidget responsive cell were equally responsive to visual stimuli and their sensory tuning properties to drifting gratings (direction, orientation and temporal frequency selectivity) were not significantly different. When comparing visual stimuli during and outside of fidget behaviors, the visually evoked responses were mostly identical. This is in contrast to a visually relevant behavior such as running, which significantly modulated visually evoked responses as reported previously. Could a subset of the same cells be involved in behavior modulation in the visual cortex? To look at this, we compared the Cohen's d of fidget and running evoked responses across the four identified neuron response types. Cells that were running modulated were in fact not modulated by fidget. In conclusion, our data supports that in passively viewing conditions, a behavioral event with no visual relevance broadly impacts the visual cortex but does not modulate visual responses. Multiple recent studies have shown that motor activity greatly impacts the activity of not only the motor cortex but primary sensory areas like V1. Given the rapid advance in brain-wide neuronal recording, there has been increasing concern related to the importance of monitoring behavioral activity to account for spontaneous brain-wide neuronal activity. Using a large two-photon Ca2+ imaging dataset collected in mice passively viewing a battery of standardized visual stimuli, we characterized the neuronal response of neurons of the visual cortex to fidgets, a single standardized motor output analogous to a startle response. We found that 47% of neurons show significant co-activity with fidgets throughout all areas and layers we recorded from. Our study confirms the importance of taking into account motor activity when analyzing neuronal data. We here propose a complementary approach to uncovering the role of motor signals in primary sensory areas. By focusing our analysis on a single standardized behavior event, we could group together a large number of behavioral trials, akin to visually-evoked stimulus trials. Closed-loop experimental designs could prove instrumental to extend this approach, by pairing specific visual stimuli with specific tracked behavioral events. Such an approach could allow a more direct analysis of how motor output modulates visual inputs. We found that excitatory neurons were responding with 3 distinct temporal profiles to fidgets. Remarkably the proportion and responses of neurons in each class was maintained in all layers and brain areas we looked at, and consequently we could not predict the location of our recording using the response to fidget despite a large database to train our decoder on. This result suggests that behavioral information is not only broadcasted broadly but also homogeneously throughout the cortical mantle. Our interpretation is that local sensory inputs shape local behavioral representations depending on the causal overlap of those events. Future research could test whether a visually conditioned behavioral event transitions from a broad modulation to be more specific to certain visual areas during learning. The broad availability of standardized behavior signals could be a key component in how the cortex selects, learns and binds local sensory information with relevant motor outputs.A standardized behavior equally impacts cortex. Computer design of our apparatus to monitor the behavior of mice during head-fixation and two-photon imaging. Example two-photon imaging field of view showcasing neurons labeled with Gcamp6f. Example video frame of mouse captured by the body camera at 30Hz. A pair of video frames showing the progression of a prototype fidget behavior in time. Computational strategy for detecting fidgets automatically. Cumulative probability sum of labelled fidget moment magnitude, showcasing the consistency of the stereotyped behavior across experiments and mice. Fidget magnitude is calculated as the sum of 2D optical flow vectors throughout the fidget duration. Fidget rate is correlated with visual stimulus type, but independent of a mouse driver-line or session number. Comparison of the fidget rate ratio between different session types across cre-lines. No significant effect was found. Average percent of video frames labeled as fidget vs. the number of experimental sessions a mouse has been exposed to, no significant learning effect found. Neuronal fidget response types are distributed equally across visual cortical layers and areas. Example two-photon imaging field of view showcasing all neurons recorded in one session. Four unique neuronal response types are displayed by the four neurons identified by colored circles. The trial-averaged z-scored activity of all neurons from one experiment, aligned to the time of fidget initiation. The activity profiles of exemplary neurons showcasing the four types of neuronal responses identified using clustering. All layer 2/3 neurons from all experiments clustered into the four neuronal response types outlined by color-coded borders. Decoding of neural response type using UMAP feature vector across cortical depths. Results suggest that the neural response types cannot be differentiated across cortical depth. Decoding of neural response type using UMAP feature vector across cortical areas. Results suggest that the neural response types cannot be differentiated across cortical area. Percent distribution of neuronal response types per cortical layer for all cortical areas. Percent distribution of the neuronal response types per cortical area for all cortical layers. Contrary to running responses, neuronal fidget responses do not modulate visually evoked activity. Example visual evoked responses of a cell to drifting gratings during running, fidgets, and resting conditions. Normalized histograms of the direction selectivity of the four cell types during the drifting gratings stimulus is plotted. The same as (b) but for cells' preferred drifting grating orientation. The same as (b) but for cells' preferred temporal frequency of drifting gratings. The mean trial ∆F/F during fidget is plotted against non-fidget behavior for each cell during its preferred direction and temporal frequency of the drifting grating stimulus. The best-fitting linear regression line is plotted in orange. This metric is plotted separately for all four cell types.The Cohen's d metric for fidget is plotted against the Cohen's d for running behavior for each cell, across all four cell types.

Transgenic mice expressing GCaMP6f were weaned and genotyped at ~p21, and surgery was performed between p37 and p63. Surgical protocols were described in previous publications associated with the two-photon datasets. Retinotopic mapping was used to delineate functionally defined visual area boundaries and enable targeting of in vivo two-photon calcium imaging to retinotopically defined locations in primary and secondary visual areas. Calcium imaging was performed using a two-photon imaging instrument, Nikon A1RMP+. The Nikon system was adapted to provide space to accommodate the behavior apparatus. Laser excitation was provided by a Ti:Sapphire laser (Chameleon Vision-Coherent) at 910nm. Pre-compensation was set at ~10,000fs2. Movies were recorded at 30Hz using resonant scanners over a 400μm field of view. Mice were head-fixed on top of a rotating disk and free to walk at will. The disk was covered with a layer of removable foam (Super-Resilient Foam, 86375K242, McMaster) to alleviate motion-induced artifacts during imaging sessions. An experiment container consisted of three imaging sessions (60 min each) at a given field of view during which mice passively observed three different stimuli. The same location was targeted for imaging on all three recording days to allow repeat comparison of the same neurons across sessions. One imaging session was performed per day, for a maximum of 16 sessions for each mouse. On the first day of imaging at a new field of view, the ISI targeting map was used to select spatial coordinates. A comparison of surface vasculature patterns was used to verify the appropriate location by imaging over a field of view of ~800μm using epi-fluorescence and blue light illumination. Once a cortical region was selected, the imaging objective was shrouded from stray light from the stimulus screen using opaque black tape. In two-photon imaging mode, the desired depth of imaging was set to record from a specific cortical depth. On subsequent imaging days, we returned to the same location by matching (1) the pattern of vessels in epi-fluorescence with (2) the pattern of vessels in two-photon imaging and (3) the pattern of cellular labeling in two-photon imaging at the previously recorded location. Calcium imaging data was collected at the four cortical depths of 175, 275, 350, and 375 micrometers. Throughout our analysis, data from the cortical depth of 175 micrometers were classified as layer 2/3, 275 and 350 micrometers as layer 4, and 375 as layer 5. Visual stimuli were generated using custom scripts written in PsychoPy as described previously. Visual stimuli included drifting gratings, static gratings, locally sparse noise, natural scenes, and natural movies. These stimuli were distributed across three ~60-minute imaging sessions. During session A, the drifting gratings, natural movie one, and natural movie three stimuli were presented. During session B, the static gratings, natural scenes, and natural movie were presented. During session C, the locally sparse noise, natural movie one, and natural movie two were presented. In each session, the different stimuli were presented in segments of 5-13 minutes and interleaved with each other. In addition, at least 5 minutes of spontaneous activity were recorded in each session. The total stimulus duration for drifting gratings was 31.5 minutes. The stimulus consisted of a full-field drifting sinusoidal grating at a single spatial frequency (0.04 cycles/degree) and contrast (80%).The grating was presented at 8 different directions (separated by 45°) and at 5 temporal frequencies (1, 2, 4, 8, 15Hz). Each grating was presented for 2 seconds, followed by 1 second of mean luminance gray before the next grating. Each grating condition (direction & temporal frequency combination) was presented 15 times, in a random order. There were blank sweeps (i.e. mean luminance gray instead of grating) presented approximately once every 20 gratings. This stimulus was used to measure the direction tuning, orientation tuning, and temporal frequency tuning of the cells.

Static gratings. The total stimulus duration was 26 minutes. The stimulus consisted of a full field static sinusoidal grating at a single contrast (80%). The grating was presented at 6 different orientations (separated by 30°), 5 spatial frequencies (0.02, 0.04, 0.08, 0.16, 0.32 cycles/degree), and 4 phases (0, 0.25, 0.5, 0.75). The grating was presented for 0.25 seconds, with no inter-grating gray period. Each grating condition (orientation, spatial frequency, and phase) was presented ~50 times in a random order. There were blank sweeps (i.e. mean luminance gray instead of grating) presented roughly once every 25 gratings. This stimulus was used to measure the spatial frequency tuning and the orientation tuning of the cells, providing a finer measurement of orientation than provided from the drifting grating stimulus.

Locally Sparse Noise stimulus. The total stimulus duration was 37.5 minutes. The Locally Sparse Noise stimulus consisted of a 16x28 array of pixels, each 4.65 degrees on a side. For each frame of the stimulus (which was presented for 0.25 seconds), a small number of pixels were white, a small number were black, and the rest were mean gray. The white and black spots were distributed such that no two spots were within 5 pixels of each other.

Natural Scenes stimulus. The stimulus consisted of 118 natural images. Images 1-58 were from the Berkeley Segmentation Dataset, images 59-101 from the van Hateren Natural Image Dataset, and images 102-118 are from the McGill Calibrated Colour Image Database. The images were presented in grayscale and were contrast normalized and resized to 1174x918 pixels. The images were presented for 0.25 seconds each, with no inter-image gray period. Each image was presented ~50 times in random order, and there were blank sweeps (i.e. mean luminance gray instead of an image) roughly once every 100 images.

Natural Movie stimulus. Three different clips were used from the opening scene of the movie Touch of Evil (Welles, 1958). Natural Movie 1 and Natural Movie 2 were both 30-second clips, while Natural Movie 3 was a 120-second clip. All clips had been contrast-normalized and were presented in grayscale at 30fps. Each movie was presented 10 times in a row with no inter-trial gray period.

During calcium imaging experiments, eye movements and animal posture were recorded. The left side of each mouse was imaged with the stimulation screen in the background to provide a detailed record of the animal response to all stimuli. The eye facing the stimulus monitor (right eye) was recorded using a custom IR imaging system. No pupillary reflex was evoked by any of these illumination LEDs.

For each two-photon imaging session, the image processing pipeline performed: (1) spatial or temporal calibration, (2) motion correction, (3) image normalization to minimize confounding random variations between sessions, (4) segmentation of connected shapes, and (5) classification of soma-like shapes from remaining clutter. The motion correction algorithm relied on phase correlation and only corrected for rigid translational errors. Each movie was partitioned into 400 consecutive frame blocks, representing 13.3s of video. Each block was registered iteratively to its own average three times. A second stage of registration integrated the periodic average frames themselves into a single global average frame through six additional iterations. The global average frame served as the reference image for the final resampling of every raw frame in the video. Fluorescence movies were processed using a segmentation algorithm to identify somatic regions of interest (ROIs) that were matched across imaging sessions. For each ROI, events were detected from ∆F/F by using an L0-regularized algorithm. For each neuron, we z-score ∆F/F trial activity and compute the mean z-scored response of each neuron aligned to the time of fidget onset (0 seconds).To determine the significance of neural activity modulation post-fidget, we apply several threshold criteria to clustered neuronal activity. The threshold criteria were used in previous calcium imaging literature, and here we present two that gave very different results: One criteria where the mean ∆F/F is larger than 6%, and one criteria where the maximum ∆F/F during the post-fidget period is greater than 5%.

We use histogram of oriented gradients (HOG) descriptors as our model features due to their invariance to position, rotation, scale, and changes to lighting between mice. Additionally, HOG vectors have been shown to perform well in dynamic behavioral classification across different individuals. Side-view full-body camera (30Hz) videos were converted to grayscale, normalized using power law compression before processing, and manually cropped to exclude background elements including the screen and rotation disk. Cropping was done by a manual selection tool that constructed a rectangle from four clicked points. Pixels outside this rectangle were cropped out. The four points were systematically chosen in this order: upper-left as the eye of the mouse, lower-left as the closest point on the running disk forming a line to the first chosen point that is parallel to the vertical axis, lower-right as the point forming a line to the second point that is parallel to the surface of the running disk, and finally upper-right as the closest point intersecting the head-stage forming a line to the third point that is parallel to the vertical axis. Histograms of Oriented Gradients (HOGs) were then computed for each frame using the following parameters: 8 histogram orientation bins, using square cells with a height of 32 pixels and 1 cell per block, and then dimensionality reduced using principal component analysis (50 dimensions). The features were then concatenated in one second blocks and fed into the model.

The training set of ~100,000 video frame labels was collected from six human annotators who used a custom XML based tool. A radial basis function support vector machine (SVM) was trained on the training set with a cross-entropy loss function (C and gamma parameters of the SVM found using a grid search).

The normalized fidget rate was computed by subtracting the baseline fidget rate during inter-stimulus grey stimulus presentation and dividing by the standard deviation of the fidget rate. To estimate fidget magnitude, an optical flow measure was computed for each grayscale cropped frame using python’s OpenCV Optical flow function. We used the Gunner’s Farneback algorithm using Two-Frame Motion Estimation Based on Polynomial Expansion with a 30 pixels kernel size. For each example, the optical flow measure was integrated over the duration of all continuous frames labeled as fidget.

Neural responses were aligned to the onset of the fidget behavior and cropped to keep 100 frames (~3 seconds) preceding the initiation of the fidget and 200 frames (~6 seconds) post-fidget initiation. Neural activity was normalized on a trial-by-trial basis by subtracting the mean activity of the 100 frames (~3 seconds) of baseline neural activity preceding the initiation of the fidget response and dividing by the standard deviation of activity. Across trial z-scored neural activity was then averaged to get the mean z-scored activity for each neuron.

The mean z-scored activity for all neurons post-fidget was passed into the k-means++ clustering algorithm with clustering evaluated using the gap statistic. The optimal number of clusters was found to be four.

To compare visually evoked responses during fidget and non-fidget, we computed the mean ∆F/F trial activity during frames annotated as fidget and non-fidget. Specifically, we only compute this metric during cells’ preferred direction and temporal frequency tuning for the drifting gratings stimulus, defined as the stimulus parameters which elicit the largest mean response.

To investigate whether there is an interaction between fidget and running modulation, we computed a Fidget Cohen’s d metric defined as:

where 𝑥1 is the mean ∆F/F during fidget, 𝑥2 is the mean ∆F/F during non-fidget and s is the norm of the standard deviation of ∆F/F during fidget and non-fidget. We then computed the running Cohen's d metric same as above but for running vs. stationary behavior. For each cell, we then plotted the fidget and running Cohen's d against each other per individual cell, and computed a covariance that indicates whether cells that were highly modulated by fidget were also highly modulated by running.

To search for any differences between fidget neural responses across different cortical layers and visual areas we took 200 frames (~6 seconds) of each neuron’s normalized average post-fidget activity and passed them into an online dimensionality reduction method, UMAP. In Python, we used the umap package (umap 0.4.0rc3, hyperparameters min_dist=0.0 and n_neighbors=200) to project the dataset into a 2-dimensional embedding to elicit any differences in the data. The embedded data points were then passed into a random forest classifier. An 85-15 train-test split was used along with 5-fold cross-validation with XGBoost (xgboost 1.2.0) and Scikit-Learn (sklearn 0.23.2). A grid hyperparameter search was used to optimize the classifier.

To counter class-imbalance in the training and test sets, each class was randomly subsampled down to the count of the least prevalent class in the set. After training, F1-scores were estimated for each class.